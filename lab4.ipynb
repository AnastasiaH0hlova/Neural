{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow 2.x\n",
    "\n",
    "1) Подготовка данных\n",
    "\n",
    "2) Использование Keras Model API\n",
    "\n",
    "3) Использование Keras Sequential + Functional API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/tutorials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для выполнения лабораторной работы необходимо установить tensorflow версии 2.0 или выше .\n",
    "\n",
    "Рекомендуется использовать возможности Colab'а по обучению моделей на GPU.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "device = '/device:gpu:1'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка данных\n",
    "Загрузите набор данных из предыдущей лабораторной работы. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 32, 32, 3)\n",
      "Train labels shape:  (49000,) int32\n",
      "Validation data shape:  (1000, 32, 32, 3)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (10000, 32, 32, 3)\n",
      "Test labels shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "def load_cifar10(num_training=49000, num_validation=1000, num_test=10000):\n",
    "    \"\"\"\n",
    "    Fetch the CIFAR-10 dataset from the web and perform preprocessing to prepare\n",
    "    it for the two-layer neural net classifier. These are the same steps as\n",
    "    we used for the SVM, but condensed to a single function.\n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 dataset and use appropriate data types and shapes\n",
    "    cifar10 = tf.keras.datasets.cifar10.load_data()\n",
    "    (X_train, y_train), (X_test, y_test) = cifar10\n",
    "    X_train = np.asarray(X_train, dtype=np.float32)\n",
    "    y_train = np.asarray(y_train, dtype=np.int32).flatten()\n",
    "    X_test = np.asarray(X_test, dtype=np.float32)\n",
    "    y_test = np.asarray(y_test, dtype=np.int32).flatten()\n",
    "\n",
    "    # Subsample the data\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    # Normalize the data: subtract the mean pixel and divide by std\n",
    "    mean_pixel = X_train.mean(axis=(0, 1, 2), keepdims=True)\n",
    "    std_pixel = X_train.std(axis=(0, 1, 2), keepdims=True)\n",
    "    X_train = (X_train - mean_pixel) / std_pixel\n",
    "    X_val = (X_val - mean_pixel) / std_pixel\n",
    "    X_test = (X_test - mean_pixel) / std_pixel\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "# If there are errors with SSL downloading involving self-signed certificates,\n",
    "# it may be that your Python version was recently installed on the current machine.\n",
    "# See: https://github.com/tensorflow/tensorflow/issues/10779\n",
    "# To fix, run the command: /Applications/Python\\ 3.7/Install\\ Certificates.command\n",
    "#   ...replacing paths as necessary.\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "NHW = (0, 1, 2)\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_cifar10()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape, y_train.dtype)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, X, y, batch_size, shuffle=False):\n",
    "        \"\"\"\n",
    "        Construct a Dataset object to iterate over data X and labels y\n",
    "        \n",
    "        Inputs:\n",
    "        - X: Numpy array of data, of any shape\n",
    "        - y: Numpy array of labels, of any shape but with y.shape[0] == X.shape[0]\n",
    "        - batch_size: Integer giving number of elements per minibatch\n",
    "        - shuffle: (optional) Boolean, whether to shuffle the data on each epoch\n",
    "        \"\"\"\n",
    "        assert X.shape[0] == y.shape[0], 'Got different numbers of data and labels'\n",
    "        self.X, self.y = X, y\n",
    "        self.batch_size, self.shuffle = batch_size, shuffle\n",
    "\n",
    "    def __iter__(self):\n",
    "        N, B = self.X.shape[0], self.batch_size\n",
    "        idxs = np.arange(N)\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(idxs)\n",
    "        return iter((self.X[i:i+B], self.y[i:i+B]) for i in range(0, N, B))\n",
    "\n",
    "\n",
    "train_dset = Dataset(X_train, y_train, batch_size=64, shuffle=True)\n",
    "val_dset = Dataset(X_val, y_val, batch_size=64, shuffle=False)\n",
    "test_dset = Dataset(X_test, y_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (64, 32, 32, 3) (64,)\n",
      "1 (64, 32, 32, 3) (64,)\n",
      "2 (64, 32, 32, 3) (64,)\n",
      "3 (64, 32, 32, 3) (64,)\n",
      "4 (64, 32, 32, 3) (64,)\n",
      "5 (64, 32, 32, 3) (64,)\n",
      "6 (64, 32, 32, 3) (64,)\n"
     ]
    }
   ],
   "source": [
    "# We can iterate through a dataset like this:\n",
    "for t, (x, y) in enumerate(train_dset):\n",
    "    print(t, x.shape, y.shape)\n",
    "    if t > 5: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Keras Model Subclassing API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Для реализации собственной модели с помощью Keras Model Subclassing API необходимо выполнить следующие шаги:\n",
    "\n",
    "1) Определить новый класс, который является наследником tf.keras.Model.\n",
    "\n",
    "2) В методе __init__() определить все необходимые слои из модуля tf.keras.layer\n",
    "\n",
    "3) Реализовать прямой проход в методе call() на основе слоев, объявленных в __init__()\n",
    "\n",
    "Ниже приведен пример использования keras API для определения двухслойной полносвязной сети. \n",
    "\n",
    "https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "(64, 10)\n"
     ]
    }
   ],
   "source": [
    "class TwoLayerFC(tf.keras.Model):\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super(TwoLayerFC, self).__init__()        \n",
    "        initializer = tf.initializers.VarianceScaling(scale=2.0,seed=42)\n",
    "        self.fc1 = tf.keras.layers.Dense(hidden_size, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc2 = tf.keras.layers.Dense(num_classes, activation='softmax',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "    \n",
    "    def call(self, x, training=False):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def test_TwoLayerFC():\n",
    "    \"\"\" A small unit test to exercise the TwoLayerFC model above. \"\"\"\n",
    "    input_size, hidden_size, num_classes = 50, 42, 10\n",
    "    x = tf.zeros((64, input_size))\n",
    "    model = TwoLayerFC(hidden_size, num_classes)\n",
    "    with tf.device('/device:gpu:0'):\n",
    "        scores = model(x)\n",
    "        print(scores.shape)\n",
    "        \n",
    "test_TwoLayerFC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте трехслойную CNN для вашей задачи классификации. \n",
    "\n",
    "Архитектура сети:\n",
    "    \n",
    "1. Сверточный слой (5 x 5 kernels, zero-padding = 'same')\n",
    "2. Функция активации ReLU \n",
    "3. Сверточный слой (3 x 3 kernels, zero-padding = 'same')\n",
    "4. Функция активации ReLU \n",
    "5. Полносвязный слой \n",
    "6. Функция активации Softmax \n",
    "\n",
    "https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/Conv2D\n",
    "\n",
    "https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreeLayerConvNet(tf.keras.Model):\n",
    "    def __init__(self, channel_1, channel_2, num_classes):\n",
    "        super(ThreeLayerConvNet, self).__init__()\n",
    "        ########################################################################\n",
    "        # TODO: Implement the __init__ method for a three-layer ConvNet. You   #\n",
    "        # should instantiate layer objects to be used in the forward pass.     #\n",
    "        ########################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        initializer = tf.initializers.VarianceScaling(scale=2.0,seed=42)\n",
    "        self.conv1 = tf.keras.layers.Conv2D(filters=channel_1,kernel_size=(5,5), activation='relu',padding='same',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.conv2 = tf.keras.layers.Conv2D(filters=channel_2,kernel_size=(3,3), activation='relu',padding='same',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.fc3 = tf.keras.layers.Dense(num_classes, activation='softmax',\n",
    "                                   kernel_initializer=initializer)\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ########################################################################\n",
    "        #                           END OF YOUR CODE                           #\n",
    "        ########################################################################\n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        scores = None\n",
    "        ########################################################################\n",
    "        # TODO: Implement the forward pass for a three-layer ConvNet. You      #\n",
    "        # should use the layer objects defined in the __init__ method.         #\n",
    "        ########################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ########################################################################\n",
    "        #                           END OF YOUR CODE                           #\n",
    "        ########################################################################        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 10)\n"
     ]
    }
   ],
   "source": [
    "def test_ThreeLayerConvNet():    \n",
    "    channel_1, channel_2, num_classes = 12, 8, 10\n",
    "    model = ThreeLayerConvNet(channel_1, channel_2, num_classes)\n",
    "    with tf.device(device):\n",
    "        x = tf.zeros((64, 3, 32, 32))\n",
    "        scores = model(x)\n",
    "        print(scores.shape)\n",
    "\n",
    "test_ThreeLayerConvNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример реализации процесса обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_part34(model_init_fn, optimizer_init_fn, num_epochs=1, is_training=False,print_every = 100):\n",
    "    \"\"\"\n",
    "    Simple training loop for use with models defined using tf.keras. It trains\n",
    "    a model for one epoch on the CIFAR-10 training set and periodically checks\n",
    "    accuracy on the CIFAR-10 validation set.\n",
    "    \n",
    "    Inputs:\n",
    "    - model_init_fn: A function that takes no parameters; when called it\n",
    "      constructs the model we want to train: model = model_init_fn()\n",
    "    - optimizer_init_fn: A function which takes no parameters; when called it\n",
    "      constructs the Optimizer object we will use to optimize the model:\n",
    "      optimizer = optimizer_init_fn()\n",
    "    - num_epochs: The number of epochs to train for\n",
    "    \n",
    "    Returns: Nothing, but prints progress during trainingn\n",
    "    \"\"\"    \n",
    "    with tf.device(device):\n",
    "\n",
    "        \n",
    "        loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "        \n",
    "        model = model_init_fn()\n",
    "        optimizer = optimizer_init_fn()\n",
    "        \n",
    "        train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "        train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "    \n",
    "        val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
    "        val_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='val_accuracy')\n",
    "        \n",
    "        t = 0\n",
    "        for epoch in range(num_epochs):\n",
    "            \n",
    "            # Reset the metrics - https://www.tensorflow.org/alpha/guide/migration_guide#new-style_metrics\n",
    "            # train_loss.reset_states()\n",
    "            # train_accuracy.reset_states()\n",
    "            \n",
    "            for x_np, y_np in train_dset:\n",
    "                with tf.GradientTape() as tape:\n",
    "                    \n",
    "                    # Use the model function to build the forward pass.\n",
    "                    scores = model(x_np, training=is_training)\n",
    "                    loss = loss_fn(y_np, scores)\n",
    "      \n",
    "                    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "                    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "                    \n",
    "                    # Update the metrics\n",
    "                    train_loss.update_state(loss)\n",
    "                    train_accuracy.update_state(y_np, scores)\n",
    "                    \n",
    "                    if t % print_every == 0:\n",
    "                        # val_loss.reset_states()\n",
    "                        # val_accuracy.reset_states()\n",
    "                        for test_x, test_y in val_dset:\n",
    "                            # During validation at end of epoch, training set to False\n",
    "                            prediction = model(test_x, training=False)\n",
    "                            t_loss = loss_fn(test_y, prediction)\n",
    "\n",
    "                            val_loss.update_state(t_loss)\n",
    "                            val_accuracy.update_state(test_y, prediction)\n",
    "                        \n",
    "                        template = 'Iteration {}, Epoch {}, Loss: {}, Accuracy: {}, Val Loss: {}, Val Accuracy: {}'\n",
    "                        print (template.format(t, epoch+1,\n",
    "                                             train_loss.result(),\n",
    "                                             train_accuracy.result()*100,\n",
    "                                             val_loss.result(),\n",
    "                                             val_accuracy.result()*100))\n",
    "                    t += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 3.0009889602661133, Accuracy: 10.9375, Val Loss: 2.950167417526245, Val Accuracy: 12.700000762939453\n",
      "Iteration 100, Epoch 1, Loss: 2.2542262077331543, Accuracy: 28.434404373168945, Val Loss: 2.4361326694488525, Val Accuracy: 24.75\n",
      "Iteration 200, Epoch 1, Loss: 2.084911823272705, Accuracy: 32.22170639038086, Val Loss: 2.2400596141815186, Val Accuracy: 29.69999885559082\n",
      "Iteration 300, Epoch 1, Loss: 2.0046002864837646, Accuracy: 34.105064392089844, Val Loss: 2.1558661460876465, Val Accuracy: 31.450000762939453\n",
      "Iteration 400, Epoch 1, Loss: 1.9358911514282227, Accuracy: 35.719295501708984, Val Loss: 2.070573091506958, Val Accuracy: 33.5\n",
      "Iteration 500, Epoch 1, Loss: 1.8904615640640259, Accuracy: 36.72343063354492, Val Loss: 2.000894546508789, Val Accuracy: 35.266666412353516\n",
      "Iteration 600, Epoch 1, Loss: 1.8595675230026245, Accuracy: 37.7261848449707, Val Loss: 1.9540117979049683, Val Accuracy: 36.4428596496582\n",
      "Iteration 700, Epoch 1, Loss: 1.8340221643447876, Accuracy: 38.48297119140625, Val Loss: 1.9141288995742798, Val Accuracy: 37.400001525878906\n"
     ]
    }
   ],
   "source": [
    "hidden_size, num_classes = 4000, 10\n",
    "learning_rate = 1e-2\n",
    "\n",
    "def model_init_fn():\n",
    "    return TwoLayerFC(hidden_size, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    return tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучите трехслойную CNN. В tf.keras.optimizers.SGD укажите Nesterov momentum = 0.9 . \n",
    "\n",
    "https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/optimizers/SGD\n",
    "\n",
    "Значение accuracy на валидационной выборке после 1 эпохи обучения должно быть > 50% ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 3.01434326171875, Accuracy: 20.3125, Val Loss: 5.303304672241211, Val Accuracy: 13.0\n",
      "Iteration 10, Epoch 1, Loss: 3.1446175575256348, Accuracy: 18.323863983154297, Val Loss: 3.7688117027282715, Val Accuracy: 16.69999885559082\n",
      "Iteration 20, Epoch 1, Loss: 2.6898348331451416, Accuracy: 20.684524536132812, Val Loss: 3.2262210845947266, Val Accuracy: 19.133333206176758\n",
      "Iteration 30, Epoch 1, Loss: 2.4940707683563232, Accuracy: 22.782258987426758, Val Loss: 2.926913261413574, Val Accuracy: 21.875\n",
      "Iteration 40, Epoch 1, Loss: 2.36868953704834, Accuracy: 25.22865867614746, Val Loss: 2.7297816276550293, Val Accuracy: 24.1200008392334\n",
      "Iteration 50, Epoch 1, Loss: 2.2780258655548096, Accuracy: 26.13357925415039, Val Loss: 2.5925514698028564, Val Accuracy: 25.433334350585938\n",
      "Iteration 60, Epoch 1, Loss: 2.204115390777588, Accuracy: 27.561477661132812, Val Loss: 2.4865174293518066, Val Accuracy: 26.914287567138672\n",
      "Iteration 70, Epoch 1, Loss: 2.1504828929901123, Accuracy: 28.653167724609375, Val Loss: 2.4016544818878174, Val Accuracy: 28.325000762939453\n",
      "Iteration 80, Epoch 1, Loss: 2.1085002422332764, Accuracy: 29.745370864868164, Val Loss: 2.3330023288726807, Val Accuracy: 29.488887786865234\n",
      "Iteration 90, Epoch 1, Loss: 2.0656373500823975, Accuracy: 30.76923179626465, Val Loss: 2.2759180068969727, Val Accuracy: 30.520000457763672\n",
      "Iteration 100, Epoch 1, Loss: 2.0252020359039307, Accuracy: 31.66769790649414, Val Loss: 2.225973606109619, Val Accuracy: 31.436363220214844\n",
      "Iteration 110, Epoch 1, Loss: 1.9945145845413208, Accuracy: 32.41835403442383, Val Loss: 2.1849441528320312, Val Accuracy: 32.133331298828125\n",
      "Iteration 120, Epoch 1, Loss: 1.9691007137298584, Accuracy: 33.070762634277344, Val Loss: 2.1441571712493896, Val Accuracy: 33.03076934814453\n",
      "Iteration 130, Epoch 1, Loss: 1.944177508354187, Accuracy: 33.695133209228516, Val Loss: 2.1079509258270264, Val Accuracy: 33.89999771118164\n",
      "Iteration 140, Epoch 1, Loss: 1.923715591430664, Accuracy: 34.38608169555664, Val Loss: 2.0762057304382324, Val Accuracy: 34.62666702270508\n",
      "Iteration 150, Epoch 1, Loss: 1.9024900197982788, Accuracy: 35.04759979248047, Val Loss: 2.050004243850708, Val Accuracy: 35.17499923706055\n",
      "Iteration 160, Epoch 1, Loss: 1.8862122297286987, Accuracy: 35.47166442871094, Val Loss: 2.0233521461486816, Val Accuracy: 35.735294342041016\n",
      "Iteration 170, Epoch 1, Loss: 1.870432734489441, Accuracy: 36.019737243652344, Val Loss: 1.9989924430847168, Val Accuracy: 36.255558013916016\n",
      "Iteration 180, Epoch 1, Loss: 1.8534895181655884, Accuracy: 36.464088439941406, Val Loss: 1.9782230854034424, Val Accuracy: 36.7315788269043\n",
      "Iteration 190, Epoch 1, Loss: 1.8387513160705566, Accuracy: 36.9273567199707, Val Loss: 1.9582622051239014, Val Accuracy: 37.22999954223633\n",
      "Iteration 200, Epoch 1, Loss: 1.8292325735092163, Accuracy: 37.103546142578125, Val Loss: 1.939130187034607, Val Accuracy: 37.661903381347656\n",
      "Iteration 210, Epoch 1, Loss: 1.8184345960617065, Accuracy: 37.38151931762695, Val Loss: 1.9239898920059204, Val Accuracy: 37.986366271972656\n",
      "Iteration 220, Epoch 1, Loss: 1.8057522773742676, Accuracy: 37.65554428100586, Val Loss: 1.9083541631698608, Val Accuracy: 38.3739128112793\n",
      "Iteration 230, Epoch 1, Loss: 1.7950186729431152, Accuracy: 37.91260528564453, Val Loss: 1.8923393487930298, Val Accuracy: 38.74583435058594\n",
      "Iteration 240, Epoch 1, Loss: 1.7848044633865356, Accuracy: 38.23262405395508, Val Loss: 1.877837061882019, Val Accuracy: 39.09600067138672\n",
      "Iteration 250, Epoch 1, Loss: 1.7764886617660522, Accuracy: 38.52714157104492, Val Loss: 1.8641867637634277, Val Accuracy: 39.376922607421875\n",
      "Iteration 260, Epoch 1, Loss: 1.765548586845398, Accuracy: 38.87092590332031, Val Loss: 1.85075044631958, Val Accuracy: 39.69629669189453\n",
      "Iteration 270, Epoch 1, Loss: 1.7552975416183472, Accuracy: 39.120155334472656, Val Loss: 1.8380463123321533, Val Accuracy: 40.05714416503906\n",
      "Iteration 280, Epoch 1, Loss: 1.747517466545105, Accuracy: 39.31272506713867, Val Loss: 1.8252776861190796, Val Accuracy: 40.406898498535156\n",
      "Iteration 290, Epoch 1, Loss: 1.7388135194778442, Accuracy: 39.583335876464844, Val Loss: 1.8135759830474854, Val Accuracy: 40.77333068847656\n",
      "Iteration 300, Epoch 1, Loss: 1.7290276288986206, Accuracy: 39.91901779174805, Val Loss: 1.8026463985443115, Val Accuracy: 41.106449127197266\n",
      "Iteration 310, Epoch 1, Loss: 1.7207505702972412, Accuracy: 40.20297622680664, Val Loss: 1.7918848991394043, Val Accuracy: 41.35312271118164\n",
      "Iteration 320, Epoch 1, Loss: 1.7141941785812378, Accuracy: 40.459503173828125, Val Loss: 1.7818483114242554, Val Accuracy: 41.60000228881836\n",
      "Iteration 330, Epoch 1, Loss: 1.7046674489974976, Accuracy: 40.77605438232422, Val Loss: 1.772149920463562, Val Accuracy: 41.838233947753906\n",
      "Iteration 340, Epoch 1, Loss: 1.696393370628357, Accuracy: 41.01906204223633, Val Loss: 1.7637163400650024, Val Accuracy: 42.06856918334961\n",
      "Iteration 350, Epoch 1, Loss: 1.6892199516296387, Accuracy: 41.1680908203125, Val Loss: 1.7551549673080444, Val Accuracy: 42.272220611572266\n",
      "Iteration 360, Epoch 1, Loss: 1.6826701164245605, Accuracy: 41.46900939941406, Val Loss: 1.7461599111557007, Val Accuracy: 42.48918914794922\n",
      "Iteration 370, Epoch 1, Loss: 1.6724932193756104, Accuracy: 41.76633834838867, Val Loss: 1.7389007806777954, Val Accuracy: 42.68157958984375\n",
      "Iteration 380, Epoch 1, Loss: 1.6650372743606567, Accuracy: 42.015254974365234, Val Loss: 1.7303006649017334, Val Accuracy: 42.92564010620117\n",
      "Iteration 390, Epoch 1, Loss: 1.656726598739624, Accuracy: 42.3073844909668, Val Loss: 1.7231159210205078, Val Accuracy: 43.119998931884766\n",
      "Iteration 400, Epoch 1, Loss: 1.6495423316955566, Accuracy: 42.50312042236328, Val Loss: 1.715912938117981, Val Accuracy: 43.273170471191406\n",
      "Iteration 410, Epoch 1, Loss: 1.6428771018981934, Accuracy: 42.696929931640625, Val Loss: 1.708984375, Val Accuracy: 43.42380905151367\n",
      "Iteration 420, Epoch 1, Loss: 1.63707435131073, Accuracy: 42.855552673339844, Val Loss: 1.7019069194793701, Val Accuracy: 43.599998474121094\n",
      "Iteration 430, Epoch 1, Loss: 1.6308239698410034, Accuracy: 43.03219223022461, Val Loss: 1.6960647106170654, Val Accuracy: 43.75227355957031\n",
      "Iteration 440, Epoch 1, Loss: 1.6255775690078735, Accuracy: 43.19728088378906, Val Loss: 1.6894694566726685, Val Accuracy: 43.913333892822266\n",
      "Iteration 450, Epoch 1, Loss: 1.620652437210083, Accuracy: 43.37236404418945, Val Loss: 1.6836292743682861, Val Accuracy: 44.010868072509766\n",
      "Iteration 460, Epoch 1, Loss: 1.6148146390914917, Accuracy: 43.53985595703125, Val Loss: 1.6772873401641846, Val Accuracy: 44.180850982666016\n",
      "Iteration 470, Epoch 1, Loss: 1.6085246801376343, Accuracy: 43.75663375854492, Val Loss: 1.6725621223449707, Val Accuracy: 44.272918701171875\n",
      "Iteration 480, Epoch 1, Loss: 1.603395938873291, Accuracy: 43.915672302246094, Val Loss: 1.6672232151031494, Val Accuracy: 44.3775520324707\n",
      "Iteration 490, Epoch 1, Loss: 1.5984256267547607, Accuracy: 44.115962982177734, Val Loss: 1.6614956855773926, Val Accuracy: 44.512001037597656\n",
      "Iteration 500, Epoch 1, Loss: 1.5940191745758057, Accuracy: 44.28642654418945, Val Loss: 1.6561108827590942, Val Accuracy: 44.61372375488281\n",
      "Iteration 510, Epoch 1, Loss: 1.5904871225357056, Accuracy: 44.42270278930664, Val Loss: 1.6510740518569946, Val Accuracy: 44.728843688964844\n",
      "Iteration 520, Epoch 1, Loss: 1.5875563621520996, Accuracy: 44.49076461791992, Val Loss: 1.6460169553756714, Val Accuracy: 44.883018493652344\n",
      "Iteration 530, Epoch 1, Loss: 1.5847885608673096, Accuracy: 44.5621452331543, Val Loss: 1.641035556793213, Val Accuracy: 45.03888702392578\n",
      "Iteration 540, Epoch 1, Loss: 1.5803016424179077, Accuracy: 44.760860443115234, Val Loss: 1.6358801126480103, Val Accuracy: 45.185455322265625\n",
      "Iteration 550, Epoch 1, Loss: 1.5766421556472778, Accuracy: 44.86445236206055, Val Loss: 1.6311689615249634, Val Accuracy: 45.3071403503418\n",
      "Iteration 560, Epoch 1, Loss: 1.5727382898330688, Accuracy: 44.97270584106445, Val Loss: 1.6265369653701782, Val Accuracy: 45.42982482910156\n",
      "Iteration 570, Epoch 1, Loss: 1.5685093402862549, Accuracy: 45.12642288208008, Val Loss: 1.621734857559204, Val Accuracy: 45.56034469604492\n",
      "Iteration 580, Epoch 1, Loss: 1.564697265625, Accuracy: 45.229129791259766, Val Loss: 1.617063283920288, Val Accuracy: 45.6898307800293\n",
      "Iteration 590, Epoch 1, Loss: 1.5612257719039917, Accuracy: 45.33629608154297, Val Loss: 1.612741231918335, Val Accuracy: 45.80666732788086\n",
      "Iteration 600, Epoch 1, Loss: 1.5583006143569946, Accuracy: 45.41389465332031, Val Loss: 1.6082592010498047, Val Accuracy: 45.92458724975586\n",
      "Iteration 610, Epoch 1, Loss: 1.553606629371643, Accuracy: 45.573341369628906, Val Loss: 1.6038048267364502, Val Accuracy: 46.053226470947266\n",
      "Iteration 620, Epoch 1, Loss: 1.5504138469696045, Accuracy: 45.70249557495117, Val Loss: 1.5998785495758057, Val Accuracy: 46.155555725097656\n",
      "Iteration 630, Epoch 1, Loss: 1.546936273574829, Accuracy: 45.8151741027832, Val Loss: 1.5956770181655884, Val Accuracy: 46.2734375\n",
      "Iteration 640, Epoch 1, Loss: 1.5439832210540771, Accuracy: 45.914588928222656, Val Loss: 1.5918123722076416, Val Accuracy: 46.38615417480469\n",
      "Iteration 650, Epoch 1, Loss: 1.5411205291748047, Accuracy: 45.996543884277344, Val Loss: 1.5879573822021484, Val Accuracy: 46.47878646850586\n",
      "Iteration 660, Epoch 1, Loss: 1.5376800298690796, Accuracy: 46.116207122802734, Val Loss: 1.584023118019104, Val Accuracy: 46.57313537597656\n",
      "Iteration 670, Epoch 1, Loss: 1.5345172882080078, Accuracy: 46.22066116333008, Val Loss: 1.5807219743728638, Val Accuracy: 46.65882110595703\n",
      "Iteration 680, Epoch 1, Loss: 1.5320931673049927, Accuracy: 46.28074645996094, Val Loss: 1.5772243738174438, Val Accuracy: 46.775360107421875\n",
      "Iteration 690, Epoch 1, Loss: 1.5290559530258179, Accuracy: 46.404666900634766, Val Loss: 1.5737042427062988, Val Accuracy: 46.88428497314453\n",
      "Iteration 700, Epoch 1, Loss: 1.5267598628997803, Accuracy: 46.49607849121094, Val Loss: 1.5708215236663818, Val Accuracy: 46.93661880493164\n",
      "Iteration 710, Epoch 1, Loss: 1.5230767726898193, Accuracy: 46.60908889770508, Val Loss: 1.5670291185379028, Val Accuracy: 47.05833053588867\n",
      "Iteration 720, Epoch 1, Loss: 1.5189590454101562, Accuracy: 46.77314376831055, Val Loss: 1.5638165473937988, Val Accuracy: 47.15342712402344\n",
      "Iteration 730, Epoch 1, Loss: 1.5163594484329224, Accuracy: 46.8536262512207, Val Loss: 1.5606420040130615, Val Accuracy: 47.26081085205078\n",
      "Iteration 740, Epoch 1, Loss: 1.5129450559616089, Accuracy: 47.00362777709961, Val Loss: 1.5570021867752075, Val Accuracy: 47.35866928100586\n",
      "Iteration 750, Epoch 1, Loss: 1.5107096433639526, Accuracy: 47.0643310546875, Val Loss: 1.5539777278900146, Val Accuracy: 47.45131301879883\n",
      "Iteration 760, Epoch 1, Loss: 1.5075520277023315, Accuracy: 47.18708801269531, Val Loss: 1.5504869222640991, Val Accuracy: 47.57143020629883\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 3e-3\n",
    "channel_1, channel_2, num_classes = 32, 16, 10\n",
    "\n",
    "def model_init_fn():\n",
    "    model = None\n",
    "    ############################################################################\n",
    "    # TODO: Complete the implementation of model_fn.                           #\n",
    "    ############################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    model = ThreeLayerConvNet(channel_1,channel_2,num_classes)\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    ############################################################################\n",
    "    #                           END OF YOUR CODE                               #\n",
    "    ############################################################################\n",
    "    return model\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    optimizer = None\n",
    "    ############################################################################\n",
    "    # TODO: Complete the implementation of model_fn.                           #\n",
    "    ############################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate,nesterov=True, momentum=0.9)\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    ############################################################################\n",
    "    #                           END OF YOUR CODE                               #\n",
    "    ############################################################################\n",
    "    return optimizer\n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn,print_every=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Использование Keras Sequential API для реализации последовательных моделей.\n",
    "\n",
    "Пример для полносвязной сети:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 3.0009889602661133, Accuracy: 10.9375, Val Loss: 2.950167417526245, Val Accuracy: 12.700000762939453\n",
      "Iteration 100, Epoch 1, Loss: 2.2542262077331543, Accuracy: 28.434404373168945, Val Loss: 2.4361326694488525, Val Accuracy: 24.75\n",
      "Iteration 200, Epoch 1, Loss: 2.084911823272705, Accuracy: 32.22170639038086, Val Loss: 2.2400596141815186, Val Accuracy: 29.69999885559082\n",
      "Iteration 300, Epoch 1, Loss: 2.0046002864837646, Accuracy: 34.105064392089844, Val Loss: 2.1558661460876465, Val Accuracy: 31.450000762939453\n",
      "Iteration 400, Epoch 1, Loss: 1.9358911514282227, Accuracy: 35.719295501708984, Val Loss: 2.070573091506958, Val Accuracy: 33.5\n",
      "Iteration 500, Epoch 1, Loss: 1.8904615640640259, Accuracy: 36.72343063354492, Val Loss: 2.000894546508789, Val Accuracy: 35.266666412353516\n",
      "Iteration 600, Epoch 1, Loss: 1.8595675230026245, Accuracy: 37.7261848449707, Val Loss: 1.9540117979049683, Val Accuracy: 36.4428596496582\n",
      "Iteration 700, Epoch 1, Loss: 1.8340221643447876, Accuracy: 38.48297119140625, Val Loss: 1.9141288995742798, Val Accuracy: 37.400001525878906\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-2\n",
    "\n",
    "def model_init_fn():\n",
    "    input_shape = (32, 32, 3)\n",
    "    hidden_layer_size, num_classes = 4000, 10\n",
    "    initializer = tf.initializers.VarianceScaling(scale=2.0,seed=42)\n",
    "    layers = [\n",
    "        tf.keras.layers.Flatten(input_shape=input_shape),\n",
    "        tf.keras.layers.Dense(hidden_layer_size, activation='relu',\n",
    "                              kernel_initializer=initializer),\n",
    "        tf.keras.layers.Dense(num_classes, activation='softmax', \n",
    "                              kernel_initializer=initializer),\n",
    "    ]\n",
    "    model = tf.keras.Sequential(layers)\n",
    "    return model\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    return tf.keras.optimizers.SGD(learning_rate=learning_rate) \n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Альтернативный менее гибкий способ обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "766/766 [==============================] - 23s 30ms/step - loss: 1.8308 - sparse_categorical_accuracy: 0.3866 - val_loss: 1.7042 - val_sparse_categorical_accuracy: 0.4100\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 1.6812 - sparse_categorical_accuracy: 0.4151\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.6812174320220947, 0.41510000824928284]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model_init_fn()\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=[tf.keras.metrics.sparse_categorical_accuracy])\n",
    "model.fit(X_train, y_train, batch_size=64, epochs=1, validation_data=(X_val, y_val))\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перепишите реализацию трехслойной CNN с помощью tf.keras.Sequential API . Обучите модель двумя способами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 3.01434326171875, Accuracy: 20.3125, Val Loss: 2.7612833976745605, Val Accuracy: 10.40000057220459\n",
      "Iteration 100, Epoch 1, Loss: 2.034350633621216, Accuracy: 28.96039581298828, Val Loss: 2.2920219898223877, Val Accuracy: 23.450000762939453\n",
      "Iteration 200, Epoch 1, Loss: 1.898222804069519, Accuracy: 33.861942291259766, Val Loss: 2.092512369155884, Val Accuracy: 29.766666412353516\n",
      "Iteration 300, Epoch 1, Loss: 1.8226604461669922, Accuracy: 36.290489196777344, Val Loss: 1.9790370464324951, Val Accuracy: 33.64999771118164\n",
      "Iteration 400, Epoch 1, Loss: 1.758641242980957, Accuracy: 38.524784088134766, Val Loss: 1.894837737083435, Val Accuracy: 36.5\n",
      "Iteration 500, Epoch 1, Loss: 1.7105251550674438, Accuracy: 40.094810485839844, Val Loss: 1.835629940032959, Val Accuracy: 38.21666717529297\n",
      "Iteration 600, Epoch 1, Loss: 1.679160714149475, Accuracy: 41.11896896362305, Val Loss: 1.7863787412643433, Val Accuracy: 39.75714111328125\n",
      "Iteration 700, Epoch 1, Loss: 1.6502171754837036, Accuracy: 42.13846206665039, Val Loss: 1.74399995803833, Val Accuracy: 41.150001525878906\n"
     ]
    }
   ],
   "source": [
    "def model_init_fn():\n",
    "    model = None\n",
    "    ############################################################################\n",
    "    # TODO: Construct a three-layer ConvNet using tf.keras.Sequential.         #\n",
    "    ############################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    input_shape = (32, 32, 3)\n",
    "    channel_1, channel_2, num_classes = 32, 16, 10\n",
    "    initializer = tf.initializers.VarianceScaling(scale=2.0,seed=42)\n",
    "    layers = [\n",
    "        tf.keras.layers.Conv2D(filters=channel_1,kernel_size=(5,5),input_shape = input_shape, activation='relu',padding='same',\n",
    "                                   kernel_initializer=initializer),\n",
    "        tf.keras.layers.Conv2D(filters=channel_2,kernel_size=(3,3), activation='relu',padding='same',\n",
    "                                   kernel_initializer=initializer),                           \n",
    "        tf.keras.layers.Flatten(input_shape=input_shape),\n",
    "        tf.keras.layers.Dense(num_classes, activation='softmax', \n",
    "                              kernel_initializer=initializer),\n",
    "    ]\n",
    "    model = tf.keras.Sequential(layers)\n",
    "    return model\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    ############################################################################\n",
    "    #                            END OF YOUR CODE                              #\n",
    "    ############################################################################\n",
    "    return model\n",
    "\n",
    "learning_rate = 5e-4\n",
    "def optimizer_init_fn():\n",
    "    optimizer = None\n",
    "    ############################################################################\n",
    "    # TODO: Complete the implementation of model_fn.                           #\n",
    "    ############################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate,nesterov=True, momentum=0.9)\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    ############################################################################\n",
    "    #                           END OF YOUR CODE                               #\n",
    "    ############################################################################\n",
    "    return optimizer\n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "766/766 [==============================] - 9s 12ms/step - loss: 1.6147 - sparse_categorical_accuracy: 0.4357 - val_loss: 1.3489 - val_sparse_categorical_accuracy: 0.5300\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.3836 - sparse_categorical_accuracy: 0.5045\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3835784196853638, 0.5044999718666077]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model_init_fn()\n",
    "model.compile(optimizer='sgd',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=[tf.keras.metrics.sparse_categorical_accuracy])\n",
    "model.fit(X_train, y_train, batch_size=64, epochs=1, validation_data=(X_val, y_val))\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Использование Keras Functional API\n",
    "\n",
    "Для реализации более сложных архитектур сети с несколькими входами/выходами, повторным использованием слоев, \"остаточными\" связями (residual connections) необходимо явно указать входные и выходные тензоры. \n",
    "\n",
    "Ниже представлен пример для полносвязной сети. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\initializers\\initializers.py:120: UserWarning: The initializer VarianceScaling is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def two_layer_fc_functional(input_shape, hidden_size, num_classes):  \n",
    "    initializer = tf.initializers.VarianceScaling(scale=2.0)\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    flattened_inputs = tf.keras.layers.Flatten()(inputs)\n",
    "    fc1_output = tf.keras.layers.Dense(hidden_size, activation='relu',\n",
    "                                 kernel_initializer=initializer)(flattened_inputs)\n",
    "    scores = tf.keras.layers.Dense(num_classes, activation='softmax',\n",
    "                             kernel_initializer=initializer)(fc1_output)\n",
    "\n",
    "    # Instantiate the model given inputs and outputs.\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=scores)\n",
    "    return model\n",
    "\n",
    "def test_two_layer_fc_functional():\n",
    "    \"\"\" A small unit test to exercise the TwoLayerFC model above. \"\"\"\n",
    "    input_size, hidden_size, num_classes = 50, 42, 10\n",
    "    input_shape = (50,)\n",
    "    \n",
    "    x = tf.zeros((64, input_size))\n",
    "    model = two_layer_fc_functional(input_shape, hidden_size, num_classes)\n",
    "    \n",
    "    with tf.device(device):\n",
    "        scores = model(x)\n",
    "        print(scores.shape)\n",
    "        \n",
    "test_two_layer_fc_functional()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 3.1505627632141113, Accuracy: 6.25, Val Loss: 3.0024468898773193, Val Accuracy: 14.399999618530273\n",
      "Iteration 100, Epoch 1, Loss: 2.211041212081909, Accuracy: 28.589109420776367, Val Loss: 2.446993589401245, Val Accuracy: 25.35000228881836\n",
      "Iteration 200, Epoch 1, Loss: 2.060417413711548, Accuracy: 32.190608978271484, Val Loss: 2.2494893074035645, Val Accuracy: 29.600000381469727\n",
      "Iteration 300, Epoch 1, Loss: 1.9887984991073608, Accuracy: 33.98567199707031, Val Loss: 2.150414228439331, Val Accuracy: 31.775001525878906\n",
      "Iteration 400, Epoch 1, Loss: 1.9216675758361816, Accuracy: 35.90632629394531, Val Loss: 2.067791700363159, Val Accuracy: 33.79999923706055\n",
      "Iteration 500, Epoch 1, Loss: 1.877763271331787, Accuracy: 37.03530502319336, Val Loss: 1.999660611152649, Val Accuracy: 35.18333435058594\n",
      "Iteration 600, Epoch 1, Loss: 1.8484934568405151, Accuracy: 37.91597366333008, Val Loss: 1.9542477130889893, Val Accuracy: 36.28571319580078\n",
      "Iteration 700, Epoch 1, Loss: 1.823294758796692, Accuracy: 38.62339401245117, Val Loss: 1.9171794652938843, Val Accuracy: 37.07500076293945\n"
     ]
    }
   ],
   "source": [
    "input_shape = (32, 32, 3)\n",
    "hidden_size, num_classes = 4000, 10\n",
    "learning_rate = 1e-2\n",
    "\n",
    "def model_init_fn():\n",
    "    return two_layer_fc_functional(input_shape, hidden_size, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    return tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поэкспериментируйте с архитектурой сверточной сети. Для вашего набора данных вам необходимо получить как минимум 70% accuracy на валидационной выборке за 10 эпох обучения. Опишите все эксперименты и сделайте выводы (без выполнения данного пункта работы приниматься не будут). \n",
    "\n",
    "Эспериментируйте с архитектурой, гиперпараметрами, функцией потерь, регуляризацией, методом оптимизации.  \n",
    "\n",
    "https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/BatchNormalization#methods https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/Dropout#methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class CustomConvNet(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(CustomConvNet, self).__init__()\n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        channel_1, channel_2, num_classes_relu, num_classes_softmax = 32, 64, 100, 10\n",
    "        initializer = tf.initializers.VarianceScaling(scale=2.0)\n",
    "\n",
    "        # Сверточный слой 1: 32 фильтра, MaxPool\n",
    "        self.conv1 = tf.keras.layers.Conv2D(channel_1, (3, 3), activation='relu', padding='same', kernel_initializer=initializer)\n",
    "        self.mp1 = tf.keras.layers.MaxPooling2D((2, 2))\n",
    "\n",
    "        # Сверточный слой 2: 64 фильтра, MaxPool\n",
    "        self.conv2 = tf.keras.layers.Conv2D(channel_2, (3, 3), activation='relu', padding='same', kernel_initializer=initializer)\n",
    "        self.mp2 = tf.keras.layers.MaxPooling2D((2, 2))\n",
    "        self.d1 = tf.keras.layers.Dropout(0.25)\n",
    "\n",
    "        # Используем flatten для сглаживания и Dropout для того, чтобы избежать переобучения\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.fc1 = tf.keras.layers.Dense(num_classes_relu, activation='relu')\n",
    "        self.d2 = tf.keras.layers.Dropout(0.25)\n",
    "        self.fc2 = tf.keras.layers.Dense(num_classes_softmax, activation='softmax')\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################\n",
    "    \n",
    "    def call(self, input_tensor, training=False):\n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        x = self.conv1(input_tensor)\n",
    "        x = self.mp1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.mp2(x)\n",
    "        x = self.d1(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.d2(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################\n",
    "        return x\n",
    "\n",
    "\n",
    "print_every = 700\n",
    "num_epochs = 10\n",
    "\n",
    "model = CustomConvNet()\n",
    "\n",
    "def model_init_fn():\n",
    "    return CustomConvNet()\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    learning_rate = 1e-3\n",
    "    return tf.keras.optimizers.Adam(learning_rate) \n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn, num_epochs=num_epochs, is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "700/700 [==============================] - 12s 15ms/step - loss: 1.3798 - sparse_categorical_accuracy: 0.5241 - val_loss: 1.2042 - val_sparse_categorical_accuracy: 0.5880\n",
      "Epoch 2/10\n",
      "700/700 [==============================] - 10s 14ms/step - loss: 1.0542 - sparse_categorical_accuracy: 0.6317 - val_loss: 0.9811 - val_sparse_categorical_accuracy: 0.6550\n",
      "Epoch 3/10\n",
      "700/700 [==============================] - 11s 16ms/step - loss: 0.9117 - sparse_categorical_accuracy: 0.6859 - val_loss: 0.9896 - val_sparse_categorical_accuracy: 0.6590\n",
      "Epoch 4/10\n",
      "700/700 [==============================] - 12s 17ms/step - loss: 0.8045 - sparse_categorical_accuracy: 0.7210 - val_loss: 0.9360 - val_sparse_categorical_accuracy: 0.6890\n",
      "Epoch 5/10\n",
      "700/700 [==============================] - 11s 16ms/step - loss: 0.7180 - sparse_categorical_accuracy: 0.7523 - val_loss: 0.9609 - val_sparse_categorical_accuracy: 0.6880\n",
      "Epoch 6/10\n",
      "700/700 [==============================] - 11s 15ms/step - loss: 0.6516 - sparse_categorical_accuracy: 0.7766 - val_loss: 1.0029 - val_sparse_categorical_accuracy: 0.6670\n",
      "Epoch 7/10\n",
      "700/700 [==============================] - 10s 14ms/step - loss: 0.5887 - sparse_categorical_accuracy: 0.7969 - val_loss: 0.9780 - val_sparse_categorical_accuracy: 0.6930\n",
      "Epoch 8/10\n",
      "700/700 [==============================] - 10s 15ms/step - loss: 0.5401 - sparse_categorical_accuracy: 0.8156 - val_loss: 1.0579 - val_sparse_categorical_accuracy: 0.6790\n",
      "Epoch 9/10\n",
      "700/700 [==============================] - 10s 14ms/step - loss: 0.4836 - sparse_categorical_accuracy: 0.8355 - val_loss: 1.0424 - val_sparse_categorical_accuracy: 0.7150\n",
      "Epoch 10/10\n",
      "700/700 [==============================] - 10s 14ms/step - loss: 0.4470 - sparse_categorical_accuracy: 0.8480 - val_loss: 1.0500 - val_sparse_categorical_accuracy: 0.7110\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 1.0857 - sparse_categorical_accuracy: 0.6948\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0856927633285522, 0.6948000192642212]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model_init_fn()\n",
    "model.compile(optimizer=optimizer_init_fn(),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=[tf.keras.metrics.sparse_categorical_accuracy])\n",
    "model.fit(X_train, y_train, batch_size=70, epochs=10, validation_data=(X_val, y_val))\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"custom_conv_net_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_2 (Bat  multiple                  12        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           multiple                  896       \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  multiple                  128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  multiple                  0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_11 (Flatten)        multiple                  0         \n",
      "                                                                 \n",
      " dense_20 (Dense)            multiple                  320400    \n",
      "                                                                 \n",
      " dense_21 (Dense)            multiple                  160400    \n",
      "                                                                 \n",
      " dense_22 (Dense)            multiple                  160400    \n",
      "                                                                 \n",
      " dense_23 (Dense)            multiple                  4010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 646246 (2.47 MB)\n",
      "Trainable params: 646176 (2.46 MB)\n",
      "Non-trainable params: 70 (280.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
